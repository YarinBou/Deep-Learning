{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "colab": {
   "name": "HW4.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OteisYToYef1"
   },
   "source": [
    "# Exercise 4\n",
    "All previous instructions hold. In addition, if you are using GPU, you must check that your code also runs on a CPU. \n",
    "\n",
    "**Make sure you use the best practices you learned in class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BYR8O3O159N"
   },
   "source": [
    "In this exercise, you will accomplish the following:\n",
    "1. Train a Localization as Regression network using a pre-trained model.\n",
    "2. Build and train a recurrent neural network that will generate text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "faxan20gYef5"
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import collections\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import os \n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import collections\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "import os \n",
    "import glob \n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math\n",
    "\n",
    "from io import open\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrxS6x_vShov",
    "outputId": "e243f5ea-9eea-4020-e1f4-22d1c71fee71"
   },
   "source": [
    "print(f\"Device is {device}\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device is cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL7SZkWcYef_"
   },
   "source": [
    "# Localization as Regression (60 points)\n",
    "\n",
    "State of the art accuracy on CIFAR10 is currently ~99% on the test set. In the next part, we will use a well known architecture called ResNet18 that was trained on ImageNet, a dataset far more rich than CIFAR10. ImageNet has 1,000 classes and 1,000,000 images and the pretrained ResNet18 available in PyTorch correctly classifies ~70% of the test set. In this part, we will use the features extracted from ResNet18 to localize and classify images of cats and dogs. \n",
    "\n",
    "Using a pretrained network as a building block for a more complicated task is at the heart of neural networks today. By leveraging the features ResNet18 extracts, we can train a model that can correctly classify and localize cats and dogs using very few images."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the data from: \"https://drive.google.com/file/d/14O0bM_h5OtYn5IJKZoFUe1bCF6N-h1Wb/view?usp=sharing\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tVKLWJyxMBIf"
   },
   "source": [
    "# use the data that was given to you with the assignment\n",
    "!unzip data.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O3JsrFDoH66g"
   },
   "source": [
    "class VOCDetection(Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 image_set='trainval',\n",
    "                 transform=None,\n",
    "                 target_transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.image_set = image_set\n",
    "        self.classes = ['cat', 'dog']\n",
    "        \n",
    "        image_dir = os.path.join(self.root, 'images')\n",
    "        annotation_dir = os.path.join(self.root, 'annotations')\n",
    "    \n",
    "        split_f = os.path.join(self.root, image_set.rstrip('\\n') + '.txt')\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip().split(' ')[0] for x in f.readlines()]\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        label, bbox = False, False\n",
    "        if(self.annotations):\n",
    "            target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot())\n",
    "            label = target['annotation']['object']['name']\n",
    "            if label == 'dog':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            x1, y1, x2, y2 = target['annotation']['object']['bndbox'].values()\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "            bbox = np.array([x1, y1,x2, y2])\n",
    "        sample = {'image':img, 'label':label, 'bbox':bbox}\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_voc_xml(self, node):\n",
    "        voc_dict = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic = collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                {ind: v[0] if len(v) == 1 else v\n",
    "                 for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        pic = sample['image']\n",
    "        sample['image'] = F.to_tensor(pic)\n",
    "        return sample\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "        \n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label, bbox = sample['image'], sample['label'], sample['bbox']\n",
    "        h, w = image.size[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = F.resize(image, (new_h, new_w))\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        bbox = bbox * [new_h / h, new_w / w, new_h / h, new_w / w]\n",
    "        bbox = bbox / 224\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
    "        return {'image':img, 'label':label, 'bbox':bbox}\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        sample['image'] = F.normalize(img, self.mean, self.std)\n",
    "        return sample\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH1wkGUyYegA"
   },
   "source": [
    "To load ResNet18 with the pretrained weights, use the following line. You are welcome to try different architectures, however they might require different input sizes or normalization.\n",
    "\n",
    "The first time you run this cell the weights will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Pog4Vr7YegA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "1001244b3b4d4dc5aad4e59fc7bfc33d"
     ]
    },
    "outputId": "1df01ed5-999e-4e85-f587-a18a5e1cc66f",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq59jdUNYegB"
   },
   "source": [
    "ResNet takes as input images of size (224,224). We will use PyTorch Transforms to change the size of the images. When ResNet18 was trained on ImageNet, the images were normalized using the mean and standard deviation of the images. In order to properly use the weights, we will use the same normalization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F2a8rSDmYegB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d2ddb52-5a63-4919-90bb-94d85e6755c7",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        Rescale((224,224)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalizing according to imagenet\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        Rescale((224,224)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "root_dir = \"/content/animals/\"\n",
    "datasets = {x: VOCDetection(root_dir, image_set=x, transform=data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "classes = datasets['train'].classes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(dataset_sizes)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2uuiMCAHYegB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5b841a54-4afb-4815-ed47-8d9e3f6d4db5",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Get a batch of training data\n",
    "sample = next(iter(dataloaders['train']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yf2qLmXyYegC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "outputId": "6b7de0b7-85d2-43a2-e450-286c4e5a6b67",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def imshow(img, label, bbox):\n",
    "    image = np.copy(img[0])\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image *= np.array([0.229, 0.224, 0.225])\n",
    "    image += np.array([0.485, 0.456, 0.406])\n",
    "    label = label[0]\n",
    "    bbox = bbox[0]\n",
    "    plt.figure();\n",
    "    fig, ax = plt.subplots(1, figsize=(12,9));\n",
    "    ax.imshow(image);\n",
    "    x1, y1, x2, y2 = bbox.numpy().reshape(-1) * 224\n",
    "    box_w, box_h = np.abs(x2-x1), np.abs(y2-y1)\n",
    "    bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, \n",
    "                             edgecolor='r', facecolor='none');\n",
    "    ax.add_patch(bbox);\n",
    "    ax.annotate(classes[label], (x1, y1), color='r', fontsize=14);\n",
    "\n",
    "imshow(sample['image'],sample['label'],sample['bbox'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HAzbDDrsYegC"
   },
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Load the pretrained ResNet-18 network and replace the top fully           #\n",
    "        # connected layer, so we could pass the features of the convolutional       #\n",
    "        # network and not only the classification layer which carries significantly #\n",
    "        # less information.                                                         #\n",
    "        # Afterwards, create a new sequential model which includes the resnet and   #\n",
    "        # add a new fully connected layer that outputs a vector with the size of    #\n",
    "        # the wanted dimensionality.                                                #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    def forward(self, images):\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the forward propagation. You need to pass an image through the     #\n",
    "        # network and extract the feature vector. In this case, when using a        #\n",
    "        # pre-defined network, you don't want to change it's weights.                #\n",
    "        # The rest of the layers you defined should accepts gradients for them to   #\n",
    "        # improve during training.                                                  #\n",
    "        # This function returns a class predication and a bounding box.             #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return pred, bbox"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14dVthUaYegC"
   },
   "source": [
    "## Guidelines\n",
    "\n",
    "1. Complete the `train_model` function in the cell below. This function takes as input the model and additional hyper-parameters, and outputs the best model found on the validation set. \n",
    "2. To babysit the learning process, **you must track the classification accuracy, IoU score and loss on the training and validation datasets and visualize them** (using tensorboard or matplotlib). I have included an implementation of the IoU metric in the notebook.\n",
    "3. Do not perform a massive grid search. Use papers, blogs, MOOCs and online guides to research best hyper-parameters for your model. Once you chose your model. Explain why you chose that architecture and why you think it performs better than other networks you tried - use citation from sources you used.\n",
    "4. You are encouraged to try Google Colab or a free AWS / Google Cloud Platform / Azure available for students. If you have an CUDA capable GPU at home - you are welcome to use it. Training one of our networks on a Core i7 for 10 epochs took 5 minutes and reached 99% classification accuracy and over 0.75 IoU score on the validation set (this took less than a minute using a RTX 2080 Ti GPU).\n",
    "5. **Include only your chosen architecture**. During experimentation, you may add as many cells as you need. Make sure to delete them before submission.\n",
    "6. Training large neural networks may take a while. Make sure your code runs reasonably fast (~30 minutes on CPU and ~5 minutes on GPU).\n",
    "7. **In order to get full marks for this section, reach at least 98% classification accuracy and a IOU score of at least 0.70 on the validation set using a single model, explain the results and include visualizations**.\n",
    "8. You are given a general skeleton for the training function. Feel free to use any different structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g0uJ_GZMYegD"
   },
   "source": [
    "def train_model(model, criterion_cls, criterion_bbox, optimizer, scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # this is how a model is copied\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0   # total loss of the network at each epoch\n",
    "            running_corrects = 0 # number of correct predictions\n",
    "            iou = 0.0            # IoU score\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for sample in dataloaders[phase]:\n",
    "                #############################################################################\n",
    "                # TO DO:                                                                    #\n",
    "                # Extract the data from the dataloader, calculate the predictions of your   #\n",
    "                # network and calculate the loss of the classification and bounding box     #\n",
    "                # prediction. When in training mode, back-prop and update the weights.      #\n",
    "                # At each epoch, calculate the test and train accuracy and IoU.             #\n",
    "                # This function returns the best model in terms of accuracy.                #\n",
    "                #############################################################################\n",
    "                pass\n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            iou = iou.item() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f}  |  Acc: {:.4f}  |  IOU: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, iou))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5sE8RbrYegD"
   },
   "source": [
    "Choose your optimizer and the loss functions for the classification and bounding box regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EkxKa-DQYegE"
   },
   "source": [
    "cnn = CNN(2)\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "criterion_cls = None\n",
    "criterion_bbox = None\n",
    "optimizer = None\n",
    "#############################################################################\n",
    "#                           START OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "GePXDneLYegE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3dcbe4be-d669-4a3a-be8e-a215ecd2ee97",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "best_model = train_model(cnn, criterion_cls, criterion_bbox, optimizer, num_epochs=10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijODw_aYegE"
   },
   "source": [
    "Once you are pleased with your results, see how your model can predict and localize cats and dogs!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "of3AdQD0YegE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "outputId": "3ce929e4-0e75-4226-f457-4d634e8e3092",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Get a batch of validation data\n",
    "sample = next(iter(dataloaders['val']))\n",
    "with torch.no_grad():\n",
    "    images = sample['image']\n",
    "    images = images.to(device)\n",
    "    label_pred, bbox_pred = best_model(images)\n",
    "    _, label_pred = torch.max(label_pred, 1)\n",
    "imshow(sample['image'], label_pred.cpu(), bbox_pred.cpu())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAoCYtwTYegE"
   },
   "source": [
    "Your visualizations here (IoU / Accuracy / Loss on training and validation datasets as a function of the epoch). Only visualize the results of your best model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mhEhi_7TYegF"
   },
   "source": [
    "# Your visualization go into this cell"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating text using RNNs (40 points)\n",
    "\n",
    "Using PyTorch, create a network that is capable of generating text, similar to the text it has seen during training. In order to tackle this problem, first read the [following blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy for some creative uses of this network and his implementation of a [char-based RNN in pure numpy](https://gist.github.com/karpathy/d4dee566867f8291f086).\n",
    "\n",
    "While implementing your own network (**in PyTorch**), make sure to consider the following:\n",
    "\n",
    "1. We need to get a large enough text file, with proper encoding. You should use the creations of [Shakespere](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt).\n",
    "2. Encode the text and map each character to an integer. One-hot encoding might also be a good idea.\n",
    "3. You might be temped to use a dataloader, however defining your own method to obtain training batches might be easier.\n",
    "4. Define your model. The following guide will help you understand how to use RNNs in PyTorch: [RNN text classification](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). You model should be relatively simple.\n",
    "5. Train your model. Training a proper model might take a while, so you are encouraged to use [Colab](https://colab.research.google.com/).\n",
    "6. Create function that takes the learned network and predicts a single character. This function should take a hidden state and an input character, and output the next hidden state and the predicted character.\n",
    "7. Finally, create a sampling function that takes the network, the required length of text to generate and an initial input and generate some text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Your code here. Add as many cells as you need ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMa0NoqbkBRx"
   },
   "source": [
    "The End"
   ]
  }
 ]
}